# -*- coding: utf-8 -*-
"""Ensemble_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OeB_WtSx-MKW_DXnx2rbDSi4mSvm60F_
"""

#ignore deprecated warnings
import warnings
warnings.filterwarnings("ignore")

#Imports
# import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
import joblib
import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt
from base64 import urlsafe_b64decode

class EnsembleAnalysis(modules.Module, modules.IOModule):

    def __init__(self):
        super(EnsembleAnalysis, self).__init__()
        self.stream = False

    def getOutput(self):
        return self.getOutputData()

    def addInput(self, moduleId, data):
        if data is None:
            return
        r = self.evaluate_stacking_model(data,
                                self.getTempFilePath('stack.pkl'),
                                self.getTempFilePath('xg_hpo.pkl'),
                                self.getTempFilePath('rf_hpo.pkl'),
                                self.getTempFilePath('et_hpo.pkl'),
                                self.getTempFilePath('dt_hpo.pkl'),
                                self.getTempFilePath('important_features.pkl'))
        print('Class for interval of network data:' + str(r))

    def start(self):
        # assumes data is {'filename', 'data'}
        # and that filenames are 'dt_hpo.pkl', 'et_hpo.pkl',
        # 'rf_hpo.pkl', 'stack.pkl', and 'xg_hpo.pkl'
        for k in self.data.keys():
            self.addTempFile(k, urlsafe_b64decode(self.data[k]))

    def stop(self):
        for k in self.data.keys():
            self.deleteTempFile(k)

    def evaluate_stacking_model(self, data, stack_model_file, xg_model_file,
                                rf_model_file, et_model_file, dt_model_file, selected_features_file):

        # Load the data
        df = data

        # Dictionary mapping index to attack name
        index_to_attack = {0:"Normal", 1: "Bot", 2: "BruteForce", 3: "DoS", 
                   4: "Infiltration", 5: "PortScan", 6: "WebAttack"}
        
        # Z-score normalization
        features = df.dtypes[df.dtypes != 'object'].index
        df[features] = df[features].apply(lambda x: (x - x.mean()) / x.std())

        # Fill empty values with 0
        df = df.fillna(0)

        #Feature Selection -> using same features extracted during training
        loaded_fs = joblib.load(selected_features_file)
        X_test = df[loaded_fs].values


        # Load models
        stack = joblib.load(stack_model_file)
        xg = joblib.load(xg_model_file)
        rf = joblib.load(rf_model_file)
        et = joblib.load(et_model_file)
        dt = joblib.load(dt_model_file)


        # Make predictions
        dt_test = dt.predict(X_test)
        et_test = et.predict(X_test)
        rf_test = rf.predict(X_test)
        xg_test = xg.predict(X_test)

        # Reshape predictions
        dt_test = dt_test.reshape(-1, 1)
        et_test = et_test.reshape(-1, 1)
        rf_test = rf_test.reshape(-1, 1)
        xg_test = xg_test.reshape(-1, 1)

        # Stack predictions
        stk_test = np.concatenate((dt_test, et_test, rf_test, xg_test), axis=1)

        # Make final prediction with stacking model
        y_predict = stack.predict(stk_test)

        # Calculate and print evaluation metrics - most common class = class of detected attack type
        class_output = np.bincount(y_predict).argmax()

        # Use the dictionary to get the attack name
        attack_name = index_to_attack.get(class_output, 'Unknown Attack')  # Default to 'Unknown Attack' if not found

        # return attack name
        return attack_name

